---
title: Reference Architecture for Pivotal Cloud Foundry on AWS
owner: Customer0
---

<strong><%= modified_date %></strong>

This guide presents a reference architecture for Pivotal Cloud Foundry (PCF) on Amazon Web Services (AWS). This architecture is valid for most production-grade PCF deployments using three availability zones (AZs). 

##<a id='base'></a> Base Reference Architecture ##

The following diagram provides an overview of a reference architecture deployment of PCF on AWS using three AZs.

<%= image_tag('aws-overview-arch.png') %>

[View a larger version of this diagram](https://raw.githubusercontent.com/pivotal-cf/docs-pcf-refarch/master/images/aws-overview-arch.png).

<p class="note"><strong>Note</strong>: Each AWS subnet must reside entirely within one AZ. As a result, a multi-AZ deployment topology requires a subnet for each AZ.</p>

### <a id="base_components"></a> Base Reference Architecture Components

The following table lists the components that are part of a base reference architecture deployment on AWS with three AZs.

<table class="nice">  
<th><strong>Component</strong></th>
<th><strong>Reference Architecture Notes</strong></th>
<tr>
 <td><strong>Domains & DNS</strong></td>  
 <td>CF Domain Zones and routes in use by the reference architecture include:<br><br>
   <ul>
     <li>domains for *.apps and *.sys (required)</li>
     <li>a route for Ops Manager (required)</li>
     <li>a route for ssh access to app containers (optional)</li>
   </ul>
   Using Route 53 to manage domains is optional.
 </td>
</tr>
<tr>
 <td><strong>Ops Manager</strong></td>  
 <td>Deployed on one of the three public subnets and accessible by FQDN or through an optional Jumpbox.</td>
</tr> 
<tr>
 <td><strong>BOSH Director</strong></td>  
 <td>Deployed on the infrastructure subnet.</td>
</tr> 
<tr>
 <td><strong>Elastic Load Balancers - HTTP, HTTPS, and SSL</strong></td>  
 <td>Required. Load balancer that handles incoming HTTP, HTTPS, and SSL traffic and forwards them to the Gorouter(s). Deployed on all three public subnets.</td>
</tr> 
<tr>
 <td><strong>Elastic Load Balancers - SSH</strong></td>  
 <td>Optional. Load balancer that provides SSH access to app containers. Deployed on all three public subnets, one per AZ.</td>
</tr> 
<tr>
 <td><strong>Gorouters</strong></td>  
 <td>Accessed through the HTTP, HTTPS, and SSL Elastic Load Balancers. Deployed on all three ERT subnets, one per AZ.</td>
</tr> 
<tr>
 <td><strong>Diego Brains</strong></td>  
 <td>This component is required. However, the SSH container access functionality is optional and enabled through the SSH Elastic Load Balancers. Deployed on all three ERT subnets, one per AZ.</td>
</tr> 
<tr>
 <td><strong>TCP Routers</strong></td>  
 <td>Optional feature for TCP routing. Deployed on all three ERT subnets, one per AZ.</td>
</tr> 
<tr>
 <td><strong>CF Database</strong></td>  
 <td>Reference architecture uses AWS RDS. Deployed on all three RDS subnets, one per AZ.</td>
</tr> 
<tr>
 <td><strong>Storage Buckets</strong></td>  
 <td>Reference architecture uses 4 S3 buckets: buildpacks, droplets, packages, and resources.</td>
</tr> 
<tr>
 <td><strong>Service Tiles</strong></td>
 <td>Deployed on all three service subnets, one per AZ.</td>
</tr>
<tr>
 <td><strong>Dynamic Services</strong></td>
 <td>Reserved for future use, dynamic services are deployed on their own subnet. Dynamic services are services autoprovisioned by BOSH based on a trigger, such as a request for that service. Pivotal recommends provisioning the multi-tenant dynamic services subnet as a /22 block.</td>
</tr>
<tr>
	<td><strong>Service User & Roles</strong></td>
	<td>One IAM role and one IAM user are recommended: the IAM role for Terraform, and the IAM user for Ops Manager and BOSH. Consult the following list:<br><br>
		<ul>
		<li>Admin Role: Terraform will use this IAM role to provision required AWS resources as well as an IAM user.</li>
		<li>IAM User: This IAM user with IAM security credentials (access key ID and secret access key) will be automatically provisioned with restrict access only to resources needed by PCF. See the AWS IAM Terraform <a href="https://github.com/pivotal-cf/pcf-pipelines/blob/master/install-pcf/aws/terraform/iam.tf">script</a> for more information.</li>
		</ul>
	</td>
</tr>
<tr>
	<td><strong>EC2 Instance Quota</strong></td>
	<td>The default EC2 instance quota on a new AWS subscription only has around 20 EC2 instances, which is not enough to host a multi-AZ deployment. The recommended quota for EC2 instances is 100. AWS requires the instances quota tickets to include Primary Instance Types, which should be t2.micro.</td>
</tr>
</table>

### <a id="network_components"></a> Network Objects

The following table lists the network objects in this reference architecture.

<table class="nice">
  <th><strong>Network Object</strong></th>
  <th><strong>Notes</strong></th>
  <th><strong>Estimated Number</strong></th>
  <tr>
  <td><strong>External Public IPs</strong></td>
  <td>One per deployment, assigned to Ops Manager.</td>
  <td>1</td> 
  </tr>
  <tr>
  <td><strong>Virtual Private Network (VPC)</strong></td>
  <td>One per deployment. A PCF deployment exists within a single VPC and a single AWS region, but should distribute PCF jobs and instances across 3 AWS AZs to ensure a high degree of availability.</td>
  <td>1</td>
  </tr>
  <tr>
  <td><strong>Subnets</strong></td>
  <td>The reference architecture requires the following subnets:
   <ul>
     <li>1 x (/24) infrastructure (BOSH Director) subnet</li>
     <li>3 x (/24) public subnets (Ops Manager, Elastic Load Balancers, NAT instances), one per AZ</li>
     <li>3 x (/20) ERT subnets (Gorouters, Diego Cells, Cloud Controllers, etc.), one per AZ</li>
     <li>3 x (/20) services subnets (RabbitMQ, MySQL, Spring Cloud Services, etc.), one per AZ</li>
     <li>3 x (/22) dynamic services subnets (used for on-demand services), one per AZ</li>
     <li>3 x (/24) RDS subnets (Cloud Controller DB, UAA DB, etc.), one per AZ.</li>
   </ul>
   For more information, see the Terraform subnets <a href="https://github.com/pivotal-cf/pcf-pipelines/blob/master/install-pcf/aws/terraform/subnets.tf">script</a>.
   </td>
  <td>16</td>
  </tr>
  <tr>
  <td><strong>Route Tables</strong></td>
  <td>This reference architecture requires 4 route tables: one for the public subnet, and one each for all 3 private subnets across 3 AZs. Consult the following list:<br><br>
  	<ul>
  	<li><strong>PublicSubnetRouteTable</strong>: This routing table enables the ingress/egress routes from/to Internet through the Internet gateway for OpsManager and the NAT Gateway.</li>
  	<li><strong>PrivateSubnetRouteTable</strong>: This routing table enables the egress routing to the Internet through the NAT Gateway for the BOSH Director and ERT.</li>
  	</ul>
    For more information, see the Terraform <a href="https://github.com/pivotal-cf/pcf-pipelines/blob/master/install-pcf/aws/terraform/route_tables.tf">script</a> that creates the route tables and the <a href="https://github.com/pivotal-cf/pcf-pipelines/blob/master/install-pcf/aws/terraform/route_table_associations.tf">script</a> that performs the route table association. 
  	<p class="note"><strong>Note</strong>: If an EC2 instance sits on a subnet with an Internet gateway attached as well as a public IP, it is accessible from the Internet through the public IP; for example, Ops Manager. ERT needs Internet access due to the access needs of using an S3 bucket as a blobstore.</p>
  </td>
  <td>4</td>
  </tr>
  <tr>
  <td><strong>Security Groups</strong></td>
  <td>The reference architecture requires 5 Security Groups. For more information, see the Terraform Security Group rules <a href="https://github.com/pivotal-cf/pcf-pipelines/blob/master/install-pcf/aws/terraform/security_group.tf">script</a>. The following table describes the Security Group ingress rules:
  	<table>
  	<tr>
  		<th>Security Group</th>
  		<th>Port</th>
  		<th>From CIDR</th>
  		<th>Protocol</th>
  		<th>Description</th>
  	</tr>
  	<tr>
  		<td>OpsMgrSG</td>
  		<td>22</td>
  		<td>0.0.0.0/0</td>
  		<td>TCP</td>
  		<td>Ops Manager SSH access</td>
  	</tr>
  	<tr>
  		<td>OpsMgrSG</td>
  		<td>443</td>
  		<td>0.0.0.0/0</td>
  		<td>TCP</td>
  		<td>Ops Manager HTTP access</td>
  	</tr>
  	<tr>
  		<td>VmsSG</td>
  		<td>ALL</td>
  		<td>VPC_CIDR</td>
  		<td>ALL</td>
  		<td>Open up connections among BOSH-deployed VMs</td>
  	</tr>
  	<tr>
  		<td>MysqlSG</td>
  		<td>3306</td>
  		<td>VPC_CIDR</td>
  		<td>TCP</td>
  		<td>Enable network access to RDS</td>
  	</tr>
  	<tr>
  		<td>ElbSG</td>
  		<td>80</td>
  		<td>0.0.0.0/0</td>
  		<td>TCP</td>
  		<td>HTTP to Elastic Runtime</td>
  	</tr>
  	<tr>
  		<td>ElbSG</td>
  		<td>443</td>
  		<td>0.0.0.0/0</td>
  		<td>TCP</td>
  		<td>HTTPS to Elastic Runtime</td>
  	</tr>
  	<tr>
  		<td>ElbSG</td>
  		<td>4443</td>
  		<td>0.0.0.0/0</td>
  		<td>TCP</td>
  		<td>WebSocket connection to Loggregator endpoint</td>
  	</tr>
  	<tr>
  		<td>SshElbSG</td>
  		<td>2222</td>
  		<td>0.0.0.0/0</td>
  		<td>TCP</td>
  		<td>SSH connection to containers</td>
  	</tr>

  	<p class="note"><strong>Note</strong>: The extra port of 4443 with the Elastic Load Balancer is due to the limitation that the Elastic Load Balancer does not support WebSocket connections on HTTP/HTTPS.</p>
  </table>
  </td>
  <td>5</td>
  </tr>
  <tr>
  <td><strong>Load Balancers</strong></td>
  <td>PCF on AWS requires the Elastic Load Balancer, which can be configured with multiple listeners to forward HTTP/HTTPS/TCP traffic. Two Elastic Load Balancers are recommended: one to forward the traffic to the Gorouters, <code>PcfElb</code>, the other to forward the traffic to the Diego Brain SSH proxy, <code>PcfSshElb</code>. For more information, see the Terraform load balancers <a href="https://github.com/pivotal-cf/pcf-pipelines/blob/master/install-pcf/aws/terraform/load_balancers.tf">script</a>.
  	<br><br>
  	The following table describes the required listeners for each load balancer:
  	<table>
  		<tr>
  			<th>ELB</th>
  			<th>Instance/Port</th>
  			<th>LB Port</th>
  			<th>Protocol</th>
  			<th>Description</th>
  		</tr>
  		<tr>
  			<td>PcfElb</td>
  			<td>gorouter/80</td>
  			<td>80</td>
  			<td>HTTP</td>
  			<td>Forward traffic to Gorouters
  		</tr>
  		<tr>
  			<td>PcfElb</td>
  			<td>gorouter/80</td>
  			<td>443</td>
  			<td>HTTPS</td>
  			<td>SSL termination and forward traffic to Gorouters</td>
  		</tr>
  		<tr>
  			<td>PcfElb</td>
  			<td>gorouter/80</td>
  			<td>4443</td>
  			<td>SSL</td>
  			<td>SSL termination and forward traffic to Gorouters</td>
  		</tr>
  		<tr>
  			<td>PcfSshElb</td>
  			<td>diego-brain/2222</td>
  			<td>2222</td>
  			<td>TCP</td>
  			<td>Forward traffic to Diego Brain for container SSH connections</td>
  		</tr>
  	</table>
  Each ELB binds with a health check to check the health of the back-end instances:
  <ul>
  	<li><code>PcfElb</code> checks the health on Gorouter port 80 with TCP</li>
  	<li><code>PcfSshElb</code> checks the health on Diego Brain port 2222 with TCP</li>
  </ul>
</td>
<td>
2
</td>
</tr>
<tr>
<td><strong>Jumpbox</strong></td>
<td>Optional. Provides a way of accessing different network components. For example, you can configure it with your own permissions and then set it up to access to Pivotal Network to download tiles. Using a Jumpbox is particularly useful in IaaSes where Ops Manager does not have a public IP. In these cases, you can SSH into Ops Manager or any other component through the Jumpbox. 
</td>
<td>1</td>
</tr>
</table>


### <a id="vpn_peering"></a> Integrate PCF with customer data center through VPN

At times, applications on PCF need to access on-premise data. The connection between an AWS VPC and an on-premise datacenter is made through [VPN peering](http://docs.aws.amazon.com/AmazonVPC/latest/NetworkAdminGuide/Introduction.html). When employing non-VPN peering, there are several points to consider:

1. Assign routable IP addresses with the following in mind: 
  * It may not be realistic to request multiple routable /22 address spaces, due to IP exhaustion.
  * Using different VPC address spaces can cause snowflakes deployments and present difficulties in automation.
  * Only make the load balancer, NAT devices, and Ops Manager routable.
  * PCF components can route egress through a NAT instance. As a result, operators do not need to assign routable IPs to PCF components.
2. Inbound traffic from the datacenter should come through an <a href="http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-internal-load-balancers.html">internal load balancer</a>.
3. Outbound traffic to the datacenter should go through AWS NAT instances.

<%= image_tag('aws-vpn.png') %>

[View a larger version of this diagram](https://raw.githubusercontent.com/pivotal-cf/docs-pcf-refarch/master/images/aws-vpn.png).
