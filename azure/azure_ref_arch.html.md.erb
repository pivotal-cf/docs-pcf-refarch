---
title: Reference Architecture for Pivotal Cloud Foundry on Azure
owner: Customer0
---

<strong><%= modified_date %></strong>

This guide presents a reference architecture for Pivotal Cloud Foundry (PCF) on Azure.

Azure does not provide resources in a way that translates directly to PCF availability zones. Instead, Azure provides high availability through fault domains and [availability sets](https://docs.microsoft.com/en-us/azure/virtual-machines/virtual-machines-windows-manage-availability).

All reference architectures described in this topic are validated for production-grade PCF deployments using fault domains and availability sets that include multiple job instances. 

See [Azure on PCF Requirements](../../customizing/azure.html) for general requirements for running PCF and specific requirements for running PCF on Azure.

## <a id="refarchs"></a>PCF Reference Architectures ##

A PCF reference architecture describes a proven approach for deploying Pivotal Cloud Foundry on a specific IaaS, such as Azure, that meets the following requirements:

* Secure
* Publicly-accessible
* Includes common PCF-managed services such as MySQL, RabbitMQ, and Spring Cloud Services
* Can host at least 100 app instances, or far more

Pivotal provides reference architectures to help you determine the best configuration for your PCF deployment.

## <a id="arch_diagram"></a> Base Azure Reference Architecture 

The following diagram provides an overview of a reference architecture deployment of PCF on Azure.

<%= image_tag('azure-overview-arch.png') %>

[View a larger version of this diagram](https://raw.githubusercontent.com/pivotal-cf/docs-pcf-refarch/master/images/azure-overview-arch.png).

### <a id="base_components"></a> Base Reference Architecture Components

The following table lists the components that are part of a base reference architecture deployment on Azure using a single resource group. 

<table class="nice">  
<th><strong>Component</strong></th>
<th><strong>Reference Architecture Notes</strong></th>
<tr>
 <td><strong>Domains and DNS</strong></td>  
 <td>CF Domain Zones and routes in use by the reference architecture include:
   <ul>
     <li>domains for *.apps and *.system (required),</li>
     <li>a route for Ops Manager (required),</li>
     <li>a route for Doppler (required),</li>
     <li>a route for Loggregator (required),</li>
     <li>a route for ssh access to app containers (optional),</li>
     <li>and a route for TCP routing to apps (optional). </li>          
   </ul>
 </td>
</tr>
<tr>
 <td><strong>Ops Manager</strong></td>  
 <td>Deployed on the infrastructure subnet and accessible by FQDN or through an optional jumpbox.</td>
</tr> 
<tr>
 <td><strong>BOSH</strong></td>  
 <td>Deployed on the infrastructure subnet.</td>
</tr> 
<tr>
 <td><strong>Azure Load Balancer - API and Apps</strong></td>  
 <td>Required. Load balancer that handles incoming API and apps requests and forwards them to the Gorouters.</td>
</tr> 
<tr>
 <td><strong>Azure Load Balancer - ssh-proxy</strong></td>  
 <td>Optional. Load balancer that provides SSH access to app containers.</td>
</tr> 
<tr>
 <td><strong>Azure Load Balancer - tcp-router</strong></td>  
 <td>Optional. Load balancer that handles TCP routing requests for apps.</td>
</tr> 
<tr>
 <td><strong>Azure Load Balancer - MySQL</strong></td>  
 <td>Required to provide high availability for MySQL backend to Pivotal Application Service (PAS).</td>
</tr> 
<tr>
 <td><strong>Gorouters</strong></td>  
 <td>Accessed through the API and Apps load balancer. Deployed on the PAS subnet, one job per Azure availability set. </td>
</tr> 
<tr>
 <td><strong>Diego Brains</strong></td>  
 <td>This component is required, however the SSH container access functionality is optional and enabled through the SSH Proxy load balancer. Deployed on the PAS subnet, one job per Azure availability set.</td>
</tr> 
<tr>
 <td><strong>TCP Routers</strong></td>  
 <td>Optional feature for TCP routing. Deployed on the PAS subnet, one job per availability zone.</td>
</tr> 
<tr>
 <td><strong>MySQL</strong></td>  
 <td>Reference architecture uses internal MySQL provided with PCF. Deployed on the PAS subnet, one job per Azure availability set.</td>
</tr> 
<tr>
 <td><strong>PAS</strong></td>  
 <td>Required. Deployed on the PAS subnet, one job per Azure availability set.</td>
</tr> 
<tr>
 <td><strong>Storage Accounts</strong></td>  
 <td>PCF on Azure requires 5 standard storage accounts: BOSH, Ops Manager, and three PAS storage accounts. Each account comes with a set amount of disk. Reference architecture recommends using 5 storage accounts because Azure Storage Accounts have an IOPs limit of approximately 20k per account, which generally relates to a BOSH JOB/VM limit of approximately 20 VMs each.</td>
</tr> 
<tr>
 <td><strong>Service Tiles</strong></td>  
 <td>Deployed on the PCF managed services subnet. Each service tile is deployed to an availability set.</td>
</tr> 
<tr>
 <td><strong>Dynamic Services</strong></td>  
 <td>Reserved for future use, dynamic services are deployed on their own subnet. Dynamic services are services autoprovisioned by BOSH based on a trigger, such as a request for that service. Pivotal recommends provisioning the multi-tenant dynamic services subnet as a /22 block.</td>
</tr>  
</table>

## <a id="common_network"></a> Alternative Network Layouts for Azure ##

This section describes the possible network layouts for PCF deployments as covered by the reference architecture of PCF on Azure. 

At a high level, there are currently two possible ways of deploying PCF as described by the reference architecture:

1. Single resource group, or
1. Multiple resource groups.

The first scenario is outlined in the [Installing PCF on Azure]. It models a single PCF deployment in a single Azure Resource Group.

If you require multiple resource groups, refer to the [Multiple Resource Group deployment](#multiple_resource_groups) section of this topic.

### <a id="base-network-layout"></a> Network Layout

This diagram illustrates the network topology of the base reference architecture for PCF on Azure. In this deployment, you expose only a minimal number of public IP addresses and deploy only one resource group.

<%= image_tag('azure-net-topology-base.png') %>

[View a larger version of this diagram](https://raw.githubusercontent.com/pivotal-cf/docs-pcf-refarch/master/images/azure-net-topology-base.png).

### <a id="network_components"></a> Network Objects

The following table lists the network objects in PCF on Azure reference architecture.

<table class="nice">
  <th><strong>Network Object</strong></th>
  <th><strong>Notes</strong></th>
  <th><strong>Estimated Number</strong></th>
  <tr>
  <td><strong>External Public IP addresses</strong></td>
  <td>Use
   <ol>
     <li>global IP address for apps and system access</li>
     <li>Ops Manager or optional jumpbox.</li>
   </ol>
   Optionally, you can use a public IP address for the ssh-proxy and tcp-router load balancers.
  </td>
  <td>1-4</td>
  </tr>
  <tr>
  <td><strong>Virtual Network</strong></td>
  <td>One per deployment. Azure virtual network objects allow multiple subnets with multiple CIDRs, so a typical deployment of PCF will likely only ever require one Azure Virtual Network object.</td>
  <td>1</td>
  </tr>
  <tr>
  <td><strong>Subnets</strong></td>
  <td>Separate subnets for
   <ol>
     <li>infrastructure (Ops Manager, Ops Manager Director, Jumpbox),</li>
     <li>PAS,</li>
     <li>services,</li>
     <li>and dynamic services.</li>
   </ol>
   Using separate subnets allows you to configure different firewall rules due to your needs.
   </td>
  <td>4</td>
  </tr>
  <tr>
  <td><strong>Routes</strong></td>
  <td>Routes are typically created by Azure dynamically when subnets are created, but you may need to create additional routes to force outbound communication to dedicated SNAT nodes. These objects are required to deploy PCF without public IP addresses.</td>
  <td>3+</td>
  </tr>
  <tr>
  <td><strong>Firewall Rules</strong></td>
  <td>Azure firewall rules are collected into a Network Security Group (NSG) and bound to a Virtual Network object and can be created to use IP ranges, subnets, or instance tags to match for source and destination fields in a rule. One NSG can be used for all firewall rules.</td>
  <td>12</td>
  </tr>
  <tr>
  <td><strong>Load Balancers</strong></td>
  <td>
  Used to handle requests to Gorouters and infrastructure components. Azure uses 1 or more load balancers. The API and Apps load balancer is required. The TCP Router load balancer used for TCP routing feature and the SSH load balancer that allows SSH access to Diego apps are both optional. In addition, you can use a MySQL load balancer to provide high availability to MySQL. This is also optional.

  The idle timeout for load balancers should be set to 30 minutes. This is to avoid a situation where the load balancer will close an idle connection without sending a RST packet, which wreaks havoc with many TCP clients. Alternatively, an F5 can be setup behind the load balancer, which can have a lower idle timeout than Azure's load balancer.
  </td>
  <td>1-4</td>
  </tr>
  <tr>
  <td><strong>Jumpbox</strong></td>
  <td>Optional. Provides a way of accessing different network components. For example, you can configure it with your own permissions and then set it up to access to Pivotal Network to download tiles. Using a jumpbox is particularly useful in IaaSes where Ops Manager does not have a public IP address. In these cases, you can SSH into Ops Manager or any other component through the jumpbox. 
</td>
  <td>1</td>
  </tr>
</table>

### <a id="multiple_resource_groups"></a> Multiple Resource Group Deployment

This diagram illustrates the case where you want to use additional resource groups in your PCF deployment on Azure.

Shared network resources may already exist in an Azure subscription. In this type of deployment, using multiple resource groups allows you to reuse existing resources instead of provisioning new ones. 

To use multiple resource groups, you need to provide the BOSH Service Principal with access to the existing network resources. 

<%= image_tag('azure-multi-resgroup.png') %>

[View a larger version of this diagram](https://raw.githubusercontent.com/pivotal-cf/docs-pcf-refarch/master/images/azure-multi-resgroup.png).

###<a id="multi-resgroup-notes"></a> Multiple Resource Groups Deployment Notes

To deploy PCF on Azure with multiple resource groups, you can define custom roles to grant resource group access to your BOSH Service Principal. For example, you might develop the following:

* Dedicated `Network` Resource Group, limits BOSH Service Principal so that it does not have admin access to network objects.

* Custom Role for BOSH Service Principal, applied to `Network` Resource Group, limits the BOSH Service Principal to minimum read-only access.
  <pre class="terminal">
  {
  "Name": "PCF Network Read Only",
  "IsCustom": true,
  "Description": "MVP PCF Read Network Resgroup",
  "Actions": [
  "Microsoft.Network/networkSecurityGroups/read",
  "Microsoft.Network/networkSecurityGroups/join/action",
  "Microsoft.Network/publicIPAddresses/read",<-- Only Required if Using Public IPs
  "Microsoft.Network/publicIPAddresses/join/action", <-- Only Required if Using Public IPs
  "Microsoft.Network/loadBalancers/read",
  "Microsoft.Network/virtualNetworks/subnets/read",
  "Microsoft.Network/virtualNetworks/subnets/join/action",
  "Microsoft.Network/virtualNetworks/read"
  ],
  "NotActions": [],
  "AssignableScopes": ["/subscriptions/[YOUR\_SUBSCRIPTION\_ID]"]
  }
  </pre>
* Custom Role for BOSH Service Principal, applied to Subscription, allowing the Operator to deploy PCF components
  <pre class="terminal">
  {
   "Name": "PCF Deploy Min Perms",
   "IsCustom": true,
   "Description": "MVP PCF Terraform Perms",
   "Actions": [
     "Microsoft.Compute/register/action"
   ],
   "NotActions": [],
   "AssignableScopes": ["/subscriptions/[YOUR\_SUBSCRIPTION\_ID]"]
  }
  </pre>

### <a id="multiple_resource_groups_multiple_load_balancer"></a> Multiple Resource Group and Multiple Load Balancer Deployment

This diagram illustrates the case where you want to deploy multiple load balancers with your multiple resource group deployment.

The key design points of this deployment are the following:

* Two Azure load balancer (ALBs) to the Gorouter exist. The first ALB is for API access, which utilizes a private IP address. The system domain should resolve to this ALB. The second ALB is for application access, which can either use a public or private IP address. The apps domain should resolve to this ALB.

* Azure does not allow the Gorouters to be members of more than one ALB member pool for the same ports, for example `80` and `443`. This restriction requires an additional reverse proxy to front the Gorouters to allow hem to expose traffic on these ports for the system domain.

<%= image_tag('azure-multi-resgroup-multi-lb.png') %>

[View a larger version of this diagram](https://raw.githubusercontent.com/pivotal-cf/docs-pcf-refarch/master/images/azure-multi-resgroup-multi-lb.png).
